

// ----------------------------------------------------------------------------------------------------------------------------- //

//  to use the motion vector for reprojection,  store it in a texture and pass it to the next frame
RWTexture2D<float4> previousFrameTexture;
RWTexture2D<float3> worldPositionTexture;


RWTexture2D<float2> motionVectorTexture; 




// ----------------------------------------------------------------------------------------------------------------------------- //

RWTexture2D<float4> Result;
float4x4 _CameraToWorld;
float4x4 _CameraInverseProjection;
float3 _WorldSpaceLightPos0;
float3 _WorldSpaceCameraPos;
float2 _ScreenParams;
float _Time = 0;
float4 _LightColor0;

// ----------------------------------------------------------------------------------------------------------------------------- //

Texture2D<float> _DepthTexture;
SamplerState sampler_DepthTexture;

Texture2D<float4> _MainTex;
SamplerState sampler_MainTex;

// Rendering
int frameCounter = 1;

// Textures
Texture3D<float4> NoiseTex;
Texture3D<float4> DetailNoiseTex;
Texture2D<float4> WeatherMap;
Texture2D<float> BlueNoise;
Texture2D<float> CloudCoverage;
Texture2D<float> HeightGradient;
Texture2D<float> DensityGradient;
Texture2D<float4> CurlNoiseTex;

SamplerState sampler_NoiseTex;
SamplerState sampler_DetailNoiseTex;
SamplerState sampler_WeatherMap;
SamplerState sampler_BlueNoise;
SamplerState sampler_CloudCoverage;
SamplerState sampler_HeightGradient;
SamplerState sampler_DensityGradient;
SamplerState sampler_CurlNoiseTex;


// Shape settings
float4 params;
int3 mapSize;
float densityMultiplier;
float densityOffset;
float scale;
float detailNoiseScale;
float detailNoiseWeight;
float3 curl_noise_weights;
float3 detailWeights;
float4 shapeNoiseWeights;
float4 phaseParams;

float density_gradient_scalar;

//Cloud Coverage:
float cloud_coverage_texture_offset = 0;
float cloud_coverage_texture_step;
float2 coverage_tiling;
float altitude_gradient_power_1;
float altitude_gradient_power_2;
float low_altitude_multiplier_influence;

// March settings
int maxLightRaySamples;
float ray_march_step_size;
float rayOffsetStrength;

float3 boundsMin;
float3 boundsMax;

float3 shapeOffset;
float3 detailOffset;

// Light settings
float powder_factor;
float lightAbsorptionTowardSun;
float lightAbsorptionThroughCloud;
float darknessThreshold;

float4 IsotropicLightTop;
float4 IsotropicLightBottom;
float  extinction_factor;

// Animation settings
float timeScale;
float baseSpeed;
float detailSpeed;



int maxViewRaySamples = 128;

// ----------------------------------------------------------------------------------------------------------------------------- //


struct Ray
{
    float3 position;
    float3 direction;
};

Ray CreateRay(float3 position, float3 direction)
{
    Ray ray;
    ray.position = position;
    ray.direction = direction;
    return ray;
}

struct DensityData
{
    float3 uvw;
    float3 gradient_uvw;
    float base_cloud_with_coverage;
    float shape_FBM;
};

DensityData CreateDensityData(float3 uvw, float base_cloud_with_coverage, float shape_FBM, float3 gradient_uvw)
{
    DensityData densityData;

    densityData.uvw = uvw;
    densityData.base_cloud_with_coverage = base_cloud_with_coverage;
    densityData.shape_FBM = shape_FBM;
    densityData.gradient_uvw = gradient_uvw;

    return densityData;
}

Ray CreateCameraRay(float2 uv)
{
    // Transform the camera origin to world space
    float3 position = mul(_CameraToWorld, float4(0.0f, 0.0f, 0.0f, 1.0f)).xyz;
    
    // Invert the perspective projection of the view-space position
    float3 direction = mul(_CameraInverseProjection, float4(uv, 0.0f, 1.0f)).xyz;
    // Transform the direction from camera to world space and normalize
    direction = mul(_CameraToWorld, float4(direction, 0.0f)).xyz;
    direction = normalize(direction);
    return CreateRay(position, direction);
}

float3 Normalize(float3 value)
{
    float length = sqrt(dot(value, value));
    return value / length;
}




// ----------------------------------------------------------------------------------------------------------------------------- //


float remap(float v, float minOld, float maxOld, float minNew, float maxNew) 
{
    return minNew + (v-minOld) * (maxNew - minNew) / (maxOld-minOld);
}


float2 squareUV(float2 uv) 
{
    float width = _ScreenParams.x;
    float height =_ScreenParams.y;
    //float minDim = min(width, height);
    float scale = 1000;
    float x = uv.x * width;
    float y = uv.y * height;
    return float2 (x/scale, y/scale);
}


// Returns (dstToBox, dstInsideBox). If ray misses box, dstInsideBox will be zero
float2 rayBoxDst(float3 boundsMin, float3 boundsMax, float3 rayOrigin, float3 invRaydir) 
{
    // Adapted from: http://jcgt.org/published/0007/03/04/
    float3 t0 = (boundsMin - rayOrigin) * invRaydir;
    float3 t1 = (boundsMax - rayOrigin) * invRaydir;
    float3 tmin = min(t0, t1);
    float3 tmax = max(t0, t1);
    
    float dstA = max(max(tmin.x, tmin.y), tmin.z);
    float dstB = min(tmax.x, min(tmax.y, tmax.z));

    // CASE 1: ray intersects box from outside (0 <= dstA <= dstB)
    // dstA is dst to nearest intersection, dstB dst to far intersection

    // CASE 2: ray intersects box from inside (dstA < 0 < dstB)
    // dstA is the dst to intersection behind the ray, dstB is dst to forward intersection

    // CASE 3: ray misses box (dstA > dstB)

    float dstToBox = max(0, dstA);
    float dstInsideBox = max(0, dstB - dstToBox);
    return float2(dstToBox, dstInsideBox);
}


// Henyey-Greenstein
float hg(float a, float g) 
{
    float g2 = g * g;
    float numerator = 1 - g2;
    float denominator = 4 * 3.1415 * pow(max(0, 1 + g2 - 2 * g * a), 1.5);
    return numerator / denominator;
}


float phase(float a) 
{
    //dual-lob Henyey-Greenstein
    float blend = .5;
    float hgBlend = hg(a,phaseParams.x) * (1-blend) + hg(a,-phaseParams.y) * blend;
    return phaseParams.z + hgBlend*phaseParams.w;
}

float beer(float d) 
{
    float beer = exp(-d);
    return beer;
}

float remap01(float v, float low, float high) 
{
    return (v-low)/(high-low);
}


DensityData sampleBaseDensity(float3 rayPos) 
{
    // Constants:
    const int mipLevel = 0;
    const float baseScale = 1/1000.0;
    const float offsetSpeed = 1/100.0;

    // Calculate texture sample positions
    float time = _Time.x * timeScale;
    float3 size = boundsMax - boundsMin;
    //size = remap(size,boundsMin, boundsMax, 0, 1);

    float3 boundsCentre = (boundsMin+boundsMax) * .5;
    float3 uvw = (size * .5 + rayPos) * baseScale * scale;
    float3 shapeSamplePos = uvw + shapeOffset * offsetSpeed + float3(time,time*0.1,time*0.2) * baseSpeed;

    // Gradient UV's
    float heightscale = size.y / 10000;                                     //size.x;
    float3 gradient_uvw = (rayPos - boundsMin) * baseScale * 0.1;
    gradient_uvw.y /= heightscale;


    // Get the Height Gradient From the Height Gradient Texture:
    float height_gradient = HeightGradient.SampleLevel(sampler_HeightGradient, gradient_uvw.yx, 0).r;

    //Get the Density Gradient From the Density Gradient Texture:
    float density_gradient = DensityGradient.SampleLevel(sampler_DensityGradient, gradient_uvw.yx, 0).r;

    
    // Get the Cloud Coverage From the Coverage Texture:
    float2 cloud_coverage_uv = float2(gradient_uvw.x * coverage_tiling.x, gradient_uvw.z * coverage_tiling.y);

    //Add extra cloud coverage for low altitude sampling:
    //float step_altitude = pow((1-gradient_uvw.y), altitude_gradient_power_2 * 0.01);
    //float low_altitude_multiplier = saturate( pow((1-density_gradient), altitude_gradient_power_1) * height_gradient * step_altitude);
    //float coverage_offset = cloud_coverage_texture_offset - low_altitude_multiplier * low_altitude_multiplier_influence; 

    float cloud_coverage = CloudCoverage.SampleLevel(sampler_CloudCoverage, cloud_coverage_uv, 0).r;
    //cloud_coverage += coverage_offset; //1-step(cloud_coverage_texture_step, cloud_coverage) + coverage_offset; 
    

    // Modify the Height Gradient using falloff at along x/z edges of the cloud container
    const float containerEdgeFadeDst = 50;
    float dstFromEdgeX = min(containerEdgeFadeDst, min(rayPos.x - boundsMin.x, boundsMax.x - rayPos.x));
    float dstFromEdgeZ = min(containerEdgeFadeDst, min(rayPos.z - boundsMin.z, boundsMax.z - rayPos.z));
    float edgeWeight = min(dstFromEdgeZ,dstFromEdgeX)/containerEdgeFadeDst;
    

    float gMin = .2;
    float gMax = .7;
    float heightPercent = (rayPos.y - boundsMin.y) / size.y;
    float heightGradient = saturate(remap(heightPercent, 0.0, gMin, 0, 1)) * saturate(remap(heightPercent, 1, gMax, 0, 1));
    height_gradient *= edgeWeight;


    //First, we build a basic cloud shape by sampling our first 3dTexture:
    float4 base_shape_noise = NoiseTex.SampleLevel(sampler_NoiseTex, shapeSamplePos, mipLevel);
    float shape_FBM = base_shape_noise.g * .625 + base_shape_noise.b * .125 + base_shape_noise.a * .25; 
    float base_cloud_density = remap(base_shape_noise.r, shape_FBM - 1., 1., 0., 1.);
    base_cloud_density = saturate(base_cloud_density + densityOffset * .1);
    base_cloud_density = remap(base_cloud_density, .85, 1., 0., 1.);
    
    base_cloud_density *= 1-cloud_coverage;

    //The next step is to multiply the result by the coverage and reduce density at the bottoms of the clouds:
    //This ensures that the bottoms will be whispy and it increases the presence of clouds in a more natural way. 
    //Remember that density increases over altitude. Now that we have our base cloud shape, we add details.

    
    // cloud shape modeled after the GPU Pro 7 chapter
    float base_cloud_with_coverage  = remap(base_cloud_density * height_gradient, cloud_coverage, 1.0, 0.0, 1.0);

    return CreateDensityData(uvw, base_cloud_with_coverage, shape_FBM, gradient_uvw);
}

float sampleDetailDensity(DensityData densityData) 
{
    // Constants:
    const int mipLevel = 0;
    const float baseScale = 1/1000.0;
    const float offsetSpeed = 1/100.0;

    // Calculate texture sample positions
    float time = _Time.x * timeScale;



    //Get the Density Gradient From the Density Gradient Texture:
    float density_gradient = DensityGradient.SampleLevel(sampler_DensityGradient, densityData.gradient_uvw.yx, 0).r;


    // Save sampling from detail tex if shape density <= 0
    if (densityData.base_cloud_with_coverage > 0) 
    {
        // Sample detail noise
        float3 detailSamplePos = densityData.uvw*detailNoiseScale + detailOffset * offsetSpeed + float3(time*.4,-time,time*0.1)*detailSpeed;
        float4 detailNoise = DetailNoiseTex.SampleLevel(sampler_DetailNoiseTex, detailSamplePos, mipLevel);

        // Sample the curl noise:
        float4 curlNoise = CurlNoiseTex.SampleLevel(sampler_CurlNoiseTex, detailSamplePos.xy, mipLevel);

        //Combine the detail and curl noise:
        detailNoise *= curlNoise;
        
        float3 normalizedDetailWeights = detailWeights / dot(detailWeights, 1);
        float detailFBM = dot(detailNoise.rgb, normalizedDetailWeights);

        // Subtract detail noise from base shape (weighted by inverse density so that edges get eroded more than centre)
        float oneMinusShape = 1 - densityData.shape_FBM;
        float detailErodeWeight = oneMinusShape * oneMinusShape * oneMinusShape;
        float cloudDensity = densityData.base_cloud_with_coverage - (1-detailFBM) * detailErodeWeight * detailNoiseWeight;

        return cloudDensity * (density_gradient * densityMultiplier * 0.1);
    }

    return 0;
}




// Calculate proportion of light that reaches the given point from the lightsource

float LightMarch(float3 position) 
{

    float3 directionToLight = _WorldSpaceLightPos0.xyz;
    float distanceInsideBox = rayBoxDst(boundsMin, boundsMax, position, 1/directionToLight).y;
    
    float stepSize = distanceInsideBox/maxLightRaySamples;
    float totalDensity = 0;

    for (int step = 0; step < maxLightRaySamples; step++) 
    {
        position += directionToLight * stepSize;

        DensityData densityDataSample = sampleBaseDensity(position);
        float detailDensity = sampleDetailDensity(densityDataSample);

        totalDensity += max(0, detailDensity * stepSize);
    }

    return totalDensity;
}


// Exponential Integral
// (http://en.wikipedia.org/wiki/Exponential_integral)
float Ei( float z )
{
    return 0.5772156649015328606065 + log( 1e-4 + abs(z) ) + z * (1.0 + z * (0.25 + z * ( (1.0/18.0) + z * ( (1.0/96.0) + z *
    (1.0/600.0) ) ) ) ); // For x!=0
}

float3 ComputeAmbientColor ( float3 _Position, float _ExtinctionCoeff )
{
    float Hp = boundsMax.y - _Position.y; // Height to the top of the volume
    float a = -_ExtinctionCoeff * Hp;
    float3 IsotropicScatteringTop = IsotropicLightTop.rgb * max( 0.0, exp( a ) - a * Ei( a ));
    float Hb = _Position.y - boundsMin.y; // Height to the bottom of the volume
    a = -_ExtinctionCoeff * Hb;
    float3 IsotropicScatteringBottom = IsotropicLightBottom.rgb * max( 0.0, exp( a ) - a * Ei( a ));
    return IsotropicScatteringTop + IsotropicScatteringBottom;
}

//http://magnuswrenninge.com/wp-content/uploads/2010/03/Wrenninge-OzTheGreatAndVolumetric.pdf

/* The main idea is to artificially lower the extinction
coefficient σt along the shadow ray to let more light reach the
shaded point. But rather than use a fixed scaling factor, we use
a summation over several scales. We also adjust the local phase
function eccentricity g and local scattering coefficient σs such
that the total contribution of light at a given point is:*/


float MultipleOctaveScattering(float density)
{
    float EXTINCTION_MULT = 1.0;
    float attenuation = 0.2;
    float contribution = 0.4;
    float phaseAttenuation = 0.1;

    const float scatteringOctaves = 4.0;

    float a = 2.0;
    float b = 2.0;
    float c = 2.0;
    float g = 0.85;

    float luminance = 0.0;

    for(float i = 0.0; i < scatteringOctaves; i++)
    {
        float phaseFunction = phase(0.3 * c);
        float beers = exp(-density * EXTINCTION_MULT * a);

        luminance += b * phaseFunction * beers;

        a *= attenuation;
        b *= contribution;
        c *= (1.0 - phaseAttenuation);
    }
    return luminance;
}




// Z buffer to linear depth
inline float LinearEyeDepth( float z )
{
    // _ZBufferParams.z = (1-far/near) / far = -9.9999
    // _ZBufferParams.w = (far / near) / far = 10
    //return 1.0 / (_ZBufferParams.z * z + _ZBufferParams.w);

    // x is (1-far/near), y is (far/near), z is (x/far) and w is (y/far).

    // https://forum.unity.com/threads/solved-what-is-lineareyedepth-doing-exactly.539791/
    //float cameraDepth = tex2D(_CameraDepthTexture, screenPos).r;
    //float eyeDepth = far * near / ((near - far) * cameraDepth + far);
    //return 10000 * 0.1 / ((0.1 - 10000) * z + 10000); 

    //return 1.0 / (-9.9999 * z + 10.0);

    return 1.0 / z;
}


float linearizeDepth(float3 worldPosition, float near, float far)
{
    float depth = length(worldPosition - _WorldSpaceCameraPos);
    return (depth - near) / (far - near);
}

float exponentializeDepth(float linearDepth, float near, float far, float exponentialFactor)
{
    linearDepth = saturate(linearDepth); // Ensure the linearDepth value is clamped between 0 and 1
    float exponentialDepth = exp(exponentialFactor * (linearDepth - 1.0)) - exp(exponentialFactor * -1.0);
    return (exponentialDepth - exp(exponentialFactor * -1.0)) / (exp(exponentialFactor) - exp(exponentialFactor * -1.0));
}

float magnitude(float2 input_vector) 
{
    return sqrt(input_vector.x * input_vector.x + input_vector.y * input_vector.y);
}



// ----------------------------------------------------------------------------------------------------------------------------- //

[numthreads(8,8,1)]

void CSMain (uint3 id : SV_DispatchThreadID)
{
    // Calculate the corresponding block position
    uint blockX = id.x / 4;
    uint blockY = id.y / 4;

    // Calculate the pixel position within the block based on the frameCounter
    uint column = (frameCounter - 1) % 4;
    uint row = (frameCounter - 1) / 4;

    // Calculate the pixel position within the 4x4 block
    uint pixelX = (id.x % 4) + column * 4 + blockX * 16;
    uint pixelY = (id.y % 4) + row * 4 + blockY * 16;
    
    uint2 current_ss_id = uint2(pixelX, pixelY);

    // Get the dimensions of the RenderTexture
    uint width, height;
    Result.GetDimensions(width, height);

    // Transform pixel to [0,1] range
    float2 uv = (current_ss_id .xy + 0.5) / float2(width, height);
    float2 uv2 = float2((current_ss_id.xy + float2(0.5f, 0.5f)) / float2(width, height) * 2.0f - 1.0f);

    // Get a ray for the UVs
    Ray ray = CreateCameraRay(uv2);

    // Camera space matches OpenGL convention where cam forward is -z. In unity forward is positive z.
    // (https://docs.unity3d.com/ScriptReference/Camera-cameraToWorldMatrix.html)
    float3 viewVector = float3(mul(_CameraInverseProjection, float4(uv2 * 2 - 1, 0, -1)).rgb);
    viewVector = float3(mul(_CameraToWorld, float4(viewVector,0)).rgb);
    float viewLength = length(viewVector);


    // Depth and cloud container intersection info:
  
    // --- Need to pass the depth texture info. Not availiable in compute shaders --- //
    float nonlin_depth =  _DepthTexture.SampleLevel(sampler_DepthTexture, uv, 0).r;
    float depth = LinearEyeDepth(nonlin_depth) * viewLength;
    float2 rayToContainerInfo = rayBoxDst(boundsMin, boundsMax, ray.position, 1/ray.direction);

    float dstToBox = rayToContainerInfo.x;
    float dstInsideBox = rayToContainerInfo.y;
    
    
    // point of intersection with the cloud container
    float3 entryPoint = ray.position + ray.direction * dstToBox;

    // ------------------------------------------------------------------------------------------------------------------------- //
    // 4x4 Bayer Matrix
    // https://www.shadertoy.com/view/sdGBzd
    // ------------------------------------------------------------------------------------------------------------------------- //


    // random starting offset (makes low-res results noisy rather than jagged/glitchy, which is nicer)
    float randomOffset = BlueNoise.SampleLevel(sampler_BlueNoise, squareUV(uv*3), 0).r;
    randomOffset *= rayOffsetStrength * 10;
    
    // Phase function makes clouds brighter around sun
    float cosAngle = dot(ray.direction, _WorldSpaceLightPos0);
    float phaseVal = phase(cosAngle);

    float dstTravelled = randomOffset;
    float dstLimit = min(depth-dstToBox, dstInsideBox);
    
    
    float stepSize = ray_march_step_size;

    // ------------------------------------------------------------------------------------------------------------------------- //
    // https://www.jpgrenier.org/clouds.html
    // To re-project the cloud volume we try to find a good approximation of the cloud's world position. 
    // While raymarching we track a weighted sum of the absorption position and generate a motion vector from it.
    // ------------------------------------------------------------------------------------------------------------------------- //

    // March through volume:

    int     sampleCount = 0;
    float   view_ray_transmittance = 1;
    float3  combined_transmittance = 0;
    float   combined_transmittance_float = 0;
    float3  previousRayPos = entryPoint + ray.direction * dstTravelled;
    float3  weightedSum = float3(0, 0, 0);

    while (dstTravelled < dstLimit) 
    {
        if(sampleCount >= maxViewRaySamples)
            break;

        ray.position = entryPoint + ray.direction * dstTravelled;
        DensityData density_data = sampleBaseDensity(ray.position);
        float base_density = density_data.base_cloud_with_coverage;

        

        
        
        // ------------------------------------------------------------------------------------------------------------------------- //
        // Sample at 2x step size until a cloud is hit (density > 0). If the density is greater than 0, 
        // then go back to the previous step and start stepping at 1x step size. If the next sample density is 0 (exit cloud),
        // then go back to sampling at 2x step size. 
        // ------------------------------------------------------------------------------------------------------------------------- //
        
        if (base_density > 0 && stepSize == ray_march_step_size) 
        {
            // Go back to the previous sample position
            ray.position = previousRayPos;
            stepSize = ray_march_step_size;
        }
        else if(base_density > 0 && stepSize == 2*ray_march_step_size)
        {

            // ------------------------------------------------------------------------------------------------------------------------- //
            // Light energy at a given sample point in the cloud as a function of
            // Energy = Attenuation * Phase * InScattering
            // Energy = exp(-density along light ray) * HG(cos(theta, eccentricity) * 1- exp(-density_sample))
            // ------------------------------------------------------------------------------------------------------------------------- //
            

            float lightDensity = 0.0;
            float detailDensity = 0.0;

            
            detailDensity = sampleDetailDensity(density_data);
            weightedSum += ray.position * detailDensity;

            
            // The powder effect should be dependent of the view direction and sun direction:
            float view_dot_light = remap(dot(Normalize(_WorldSpaceCameraPos.xyz), Normalize(_WorldSpaceLightPos0.xyz)),-1,1,0,1);

            // Calculate how much light reaches the current sample point from the light source (Beer):

            // If and only if the result of this expensive sample is non-zero, we take the  additional samples along the ray from the sample point to the sun.
            if(detailDensity > 0)
                lightDensity = LightMarch(ray.position);

            float light_transmittance_beer = MultipleOctaveScattering(lightDensity * lightAbsorptionTowardSun);
            light_transmittance_beer = darknessThreshold + light_transmittance_beer * (1-darknessThreshold);


            // Apply the equation again for the path along the ray to the viewer (Beer-Powder):
            float view_ray_transmittance_beer = MultipleOctaveScattering(detailDensity* stepSize * lightAbsorptionThroughCloud);
            float view_ray_transmittance_powder = 1.0 - MultipleOctaveScattering(detailDensity * stepSize * 2)  *view_dot_light  * powder_factor * .01;

            float view_ray_transmittance_beer_powder = view_ray_transmittance_beer * view_ray_transmittance_powder;
            view_ray_transmittance_beer_powder = darknessThreshold + view_ray_transmittance_beer_powder * (1-darknessThreshold);


            view_ray_transmittance *= view_ray_transmittance_beer_powder;

            float extinction_coefficent = extinction_factor * detailDensity;
            float3 ambient_color = ComputeAmbientColor(ray.position, extinction_coefficent);

            
            // Combined transmittance:
            combined_transmittance += (detailDensity * stepSize) * view_ray_transmittance * light_transmittance_beer * phaseVal * ambient_color;
            
            //Once the alpha of the image reaches 1 we don’t need to keep sampling so we stop the march early:
            combined_transmittance_float += (detailDensity * stepSize) * view_ray_transmittance * light_transmittance_beer * phaseVal;

            if(combined_transmittance_float >= 1.0)
            {
                break;
            }
            
            // Exit early if T is close to zero as further samples won't affect the result much:
            
            if (view_ray_transmittance < 0.01) 
            {
                break;
            }

        }
        else
        {
            stepSize = ray_march_step_size * 2;
        }

        // Update the previous sample position
        dstTravelled += stepSize;
        previousRayPos = ray.position;
        sampleCount++;
    }
    
 
    //float3 motionVector = (weightedSum - prevWeightedSum) / (combined_transmittance_float - prevCombinedTransmittanceFloat);
    
    //float l_depth = linearizeDepth(estimatedWorldPosition, nearPlane, farPlane);
    //float e_depth = exponentializeDepth(l_depth, nearPlane, farPlane, 2.0);
    
   

    // ------------------------------------------------------------------------------------------------------------------------- //
    // Once we have the world position, we can reproject into the previous frame using the previous frame model view matrix
    // q_cs - mul(VP_prev, p);
    // q_uv = 0.5 * (q_cs.xy / q_cs.w) + 0.5;

    // The model, view and projection matrices are three separate matrices. 
    // Model maps from an object's local coordinate space into world space, 
    // view from world space to camera space, projection from camera to screen.

    // Then we have q_uv, which we can use to lookup in our history buffer.
    // history sample is then, c_hist = sample(buf_history, q_uv);

    // for dynamics scenes we need a velocity buffer
    // - seperate pass before temporal
    // - initialize to camera motion using static reprojection
    // - v = p_uv - q_uv
    // - then render dynamic objects on top with a biasing test
    // - v = compute_ssvel( p, q, VP, VP_Prev)

    // reprojection step, then becomes read and subtract
    // - v = sample( buf_velocity, p_uv)
    // q_uv = p_uv - v

    // but we dont actually sample the velocity directly in the current fragement, because then fragments on edges of 
    // objects wont travel with the occluder. Instead, use the velocity of the closest (depth) fragment within 3x3 region.
    // v = sample (buf_velocity, closest_fragment(p_uv).xy)



    /*If you are updating a different pixel in each frame, you won't have access to the exact world position of the corresponding 
    pixel from the previous frame. In that case, you would need to estimate the motion vector using alternative methods.
    One approach could be to use a spatial or temporal coherence technique. Spatial coherence assumes that neighboring pixels have 
    similar motion, while temporal coherence assumes that the motion between consecutive frames is relatively smooth. Here's a general 
    outline of how you can implement this: 
    - In each frame, render the volumetric scene and update a different pixel within each 4x4 block.
    - Keep track of the estimated world position of the current frame's pixel.
    - Use the estimated world position of the current frame's pixel and its neighboring pixels from the previous frame to estimate the motion vector. 
      You can interpolate or extrapolate the motion vectors of neighboring pixels based on their previous motion vectors.
    - Convert the estimated motion vector to the desired range (e.g., -1 to 1 or 0 to 1) based on your requirements.
    - Use the estimated motion vector in your volumetric rendering algorithm to apply motion-based effects.
    It's important to note that the accuracy of the motion vectors will depend on the quality of the estimation technique and the 
    assumption of spatial or temporal coherence. Adjustments and refinements may be necessary to achieve satisfactory results based on 
    your specific scenario. Keep in mind that updating only one pixel per 4x4 block each frame will still result in lower accuracy and 
    potentially higher temporal artifacts compared to updating the entire block.*/

    // ------------------------------------------------------------------------------------------------------------------------- //
    
    // Idk what im doing here:


    // estimate the current pixels world position
    float3 estimated_ws_pos = weightedSum / combined_transmittance_float;
    worldPositionTexture[current_ss_id] = estimated_ws_pos;

    
    // reproject into previous frame
    float4 reproject_cs_pos = mul(_PrevVP_NoFlip, estimated_ws_pos);
    float2 reproject_ss_ndc = reproject_cs_pos.xy / reproject_cs_pos.w;
    float2 reproject_ss_id = 0.5 * reproject_ss_ndc + 0.5;

    // initialize to camera motion using static reprojection.
    
    if(magnitude(motionVectorTexture[current_ss_id]) == 0.0)
    {
        float2 velocity_ss = current_ss_id - reproject_ss_id;
        motionVectorTexture[current_ss_id] = velocity_ss;
    }
    else
    {
        // Assume Spatial coherence, meaning that neighboring pixels have similar motion

        // get maximum velocity from neighboring pixels [3x3 grid pattern] as the estimate for the current pixels velocity
        float2 max_velocity = 0.0;
        float dot_max_velocity = 0.0;
        
        [unroll]
        for (int i = -1; i <= 1; i++)
        {
            [unroll]
            for (int j = -1; j <= 1; j++)
            {
                float2 velocity = motionVectorTexture[uint2(reproject_ss_id.x + i, reproject_ss_id.y + j)];
                float dot_velocity = dot(velocity, velocity);

                if (dot_velocity > dot_max_velocity)
                {
                    max_velocity =  velocity;
                    dot_max_velocity = dot_velocity;
                }
            }
        }
        motionVectorTexture[current_ss_id] = max_velocity;
    }
       

    
    

   

    
    


    


    
    

   // samplePositionUV = samplePositionUV - velocity;

   
    //float3 sampleHistory = previousFrameTexture[samplePositionUV];







    // Add clouds to background 
    float3 backgroundCol = _MainTex.SampleLevel(sampler_MainTex, uv, 0).rgb;
    float3 cloudCol = combined_transmittance * _LightColor0.rgb;
    float3 col = backgroundCol * view_ray_transmittance + cloudCol;
    

    previousFrameTexture[current_ss_id] = float4(col,1);
    Result[current_ss_id] = float4(col,1);
}

